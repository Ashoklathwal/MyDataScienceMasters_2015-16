help("read.csv")
?read.table
getwd()
install.packages("swirl")
library("swirl")
swirl()
5 + 7
x <- 5 + 7
x
y <- x - 3
y
z <- c(1.1, 9, 3.14)
?c
z
c(z, 555, z)
z * 2 + 100
my_sqrt <- sqrt(z - 1)
mysqrt
my_sqrt
my_div <- z / my_sqrt
my_div
c(1, 2, 3, 4) + c(0, 10)
c(1, 2, 3, 4) + c(0, 10, 100)
c(1, 2, 3, 4) + c(0, 10, 100)
c(1, 2, 3, 4) + c(0, 10, 100)
z * 2 + 1000
my_div
swirl(0)
getwd()
ls()
x <- 9
ls()
list.files()
dir()
?list.files
args(list.dirs())
args(list.files)
old.dir <- getwd()
dir.create("testdir")
setwd("testdir")
file.create
file.create("mytest.R")
dir()
file.exists("mytest.R")
file.info("mytest.R")
file.rename("mytest.R", "mytest2.R")
file.copy("mytest2.R", "mytest3.R")
file.path("mytest3.R")
file.path("folder1", "folder2")
?dir.create
dir.create
dir.create(file.path("testdir2/testdir3"), recursive = TRUE)
dir.create(file.path("testdir2", "testdir3"), recursive = TRUE)
unlink("testdir2", recursive = TRUE)
setwd(old.dir)
unlink("testdir", recursive = TRUE)
1:20
pi:10
15:1
?':'
seq(1:2)
seq(1, 20)
seq(0, 10, by=0.5)
seq(5, 10, length=30)
my_seq <- seq(5, 10, length=30)
length(my_seq)
1:length(my_seq)
seql(along.with = my_seq)
seq(along.with = my_seq)
seq_along(my_seq)
rep(0, times = 40)
rep(c(0, 1, 2))
rep(c(0, 1, 2), times = 10)
rep(c(0, 1, 2), each = 10)
library(caret);
library(caret);
library(randomForest);
library(rpart);
library(pls);
library(plyr);
library(knitr);
library(gbm);
library(kknn);
fTrainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv";
fTestUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv";
fTrainDest <- "./data/pml-training.csv";
if (!file.exists(fTrainDest)){
download.file(fTrainUrl, destfile=fTrainDest, method="curl");
}
fTestDest <- "./data/pml-testing.csv"
if (!file.exists(fTestDest)){
download.file(fTestUrl, destfile=fTestDest, method="curl");
}
## load data
trainData <- read.csv(fTrainDest, na.strings = c("NA", "", "#DIV/0!"));
testData <- read.csv(fTestDest, na.strings = c("NA", "", "#DIV/0!"));
## Do coherent check
all.equal(colnames(testData)[1:length(colnames(testData)) - 1],
colnames(trainData)[1:length(colnames(trainData)) - 1]);
## Remove non-zero variance
colNZV <- nearZeroVar(trainData, saveMetrics = TRUE);
trainData <- trainData[, !colNZV$nzv];
#Variables with more than 50% missing values are removed
removeVars <- function(v){
if(sum(is.na(trainData[, v])) > 0.50 * nrow(trainData) ){
return(TRUE)
}else{
return(FALSE)
}
}
rmVars <- sapply(colnames(trainData), removeVars);
trainData <- trainData[, !rmVars];
## Remove features not related to prediction like id, timestamp and names
trainData <- trainData[, -c(1:6)];
testData <- testData[, -c(1:6)];
set.seed(54321);
tc <- trainControl(method = "repeatedcv",
repeats = 3,
classProbs = TRUE,
allowParallel=TRUE);
rfFit <- train(classe ~ ., data = trainData, method = "rf", trControl = tc);
plsdaFit <- train(classe ~ ., data = trainData, method = "pls", trControl = tc);
gbmFit <- train(classe ~ ., data = trainData, method = "gbm",
verbose = FALSE, trControl = tc);
knnFit <- train(classe ~ ., data = trainData, method = "kknn", trControl = tc);
Methods <- c("Random forest", "PLSDA", "Gradient boosting machine", "k-Nearest Neighbors");
Accuracy <- c(max(rfFit$results$Accuracy),
max(plsdaFit$results$Accuracy),
max(gbmFit$results$Accuracy),
max(knnFit$results$Accuracy));
perf <- cbind(Methods, Accuracy);
perf;
rfPred <- predict(rfFit, testData);
setwd("~/workplace/Practical_Machine_Learning")
library(caret);
library(randomForest);
library(rpart);
library(pls);
library(plyr);
library(knitr);
library(gbm);
library(kknn);
fTrainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv";
fTestUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv";
fTrainDest <- "./data/pml-training.csv";
if (!file.exists(fTrainDest)){
download.file(fTrainUrl, destfile=fTrainDest, method="curl");
}
fTestDest <- "./data/pml-testing.csv"
if (!file.exists(fTestDest)){
download.file(fTestUrl, destfile=fTestDest, method="curl");
}
## load data
trainData <- read.csv(fTrainDest, na.strings = c("NA", "", "#DIV/0!"));
testData <- read.csv(fTestDest, na.strings = c("NA", "", "#DIV/0!"));
## Do coherent check
all.equal(colnames(testData)[1:length(colnames(testData)) - 1],
colnames(trainData)[1:length(colnames(trainData)) - 1]);
## Remove non-zero variance
colNZV <- nearZeroVar(trainData, saveMetrics = TRUE);
trainData <- trainData[, !colNZV$nzv];
#Variables with more than 50% missing values are removed
removeVars <- function(v){
if(sum(is.na(trainData[, v])) > 0.50 * nrow(trainData) ){
return(TRUE)
}else{
return(FALSE)
}
}
rmVars <- sapply(colnames(trainData), removeVars);
trainData <- trainData[, !rmVars];
## Remove features not related to prediction like id, timestamp and names
trainData <- trainData[, -c(1:6)];
testData <- testData[, -c(1:6)];
set.seed(54321);
tc <- trainControl(method = "repeatedcv",
repeats = 3,
classProbs = TRUE,
allowParallel=TRUE);
rfFit <- train(classe ~ ., data = trainData, method = "rf", trControl = tc);
## Load libraries
library(caret);
library(randomForest);
library(rpart);
library(pls);
library(plyr);
library(knitr);
library(gbm);
library(kknn);
fTrainUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv";
fTestUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv";
fTrainDest <- "./data/pml-training.csv";
if (!file.exists(fTrainDest)){
download.file(fTrainUrl, destfile=fTrainDest, method="curl");
}
fTestDest <- "./data/pml-testing.csv"
if (!file.exists(fTestDest)){
download.file(fTestUrl, destfile=fTestDest, method="curl");
}
trainData <- read.csv(fTrainDest, na.strings = c("NA", "", "#DIV/0!"));
testData <- read.csv(fTestDest, na.strings = c("NA", "", "#DIV/0!"));
## Do coherent check
all.equal(colnames(testData)[1:length(colnames(testData)) - 1],
colnames(trainData)[1:length(colnames(trainData)) - 1]);
## Remove non-zero variance
columns <- nearZeroVar(trainData, saveMetrics = TRUE);
trainData <- trainData[, !columns$nzv];
## Remove variables with more than 50% missing values
removeVars <- function(v){
if(sum(is.na(trainData[, v])) > 0.50 * nrow(trainData) ){
return(TRUE)
}else{
return(FALSE)
}
}
rmVars <- sapply(colnames(trainData), removeVars);
trainData <- trainData[, !rmVars];
trainData <- trainData[, -(1:6)];
testData <- testData[, -(1:6)];
set.seed(54321);
tc <- trainControl(method = "repeatedcv",
repeats = 3,
classProbs = TRUE,
allowParallel=TRUE);
rfModel <- train(classe ~ ., data = trainData, method = "rf", trControl = tc);
library('pls')
install.packages('pls')
install.packages('knn')
library(ElemStatLearn)
install.packages('ElemStatLearn')
install.packages("ElemStatLearn")
# question 1
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
y <-factor(vowel.train$y)
y <-factor(vowel.test$y)
set.seed(33833)
modelrf <- train(y~.,method="rf", data=vowel.train)
modelgbm <- train(y~.,method="gbm", data=vowel.train)
predrf <- predict(modelrf,newdata=vowel.test)
predgbm <- predict(modelgbm,newdata=vowel.test)
confusionMatrix(predrf,vowel.test$y)
confusionMatrix(predgbm,vowel.test$y)
library(ElemStatLearn)
library(caret)
data(vowel.train)
data(vowel.test)
y <-factor(vowel.train$y)
y <-factor(vowel.test$y)
set.seed(33833)
modelrf <- train(y~.,method="rf", data=vowel.train)
modelgbm <- train(y~.,method="gbm", data=vowel.train)
predrf <- predict(modelrf,newdata=vowel.test)
predgbm <- predict(modelgbm,newdata=vowel.test)
confusionMatrix(predrf,vowel.test$y)
confusionMatrix(predgbm,vowel.test$y)
predrf
confusionMatrix(predrf,vowel.test$y)
library(ElemStatLearn)
library(caret)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
modelrf <- train(y~.,method="rf", data=vowel.train)
modelgbm <- train(y~.,method="gbm", data=vowel.train)
predrf <- predict(modelrf,newdata=vowel.test)
predgbm <- predict(modelgbm,newdata=vowel.test)
confusionMatrix(predrf,vowel.test$y)
confusionMatrix(predgbm,vowel.test$y)
rf_accuracy = sum(predrf == vowel.test$y) / length(predrf)
gbm_accuracy = sum(predgbm == vowel.test$y) / length(predgbm)
rf_accuracy
gbm_accuracy
agree <- vowel.test[predrf == predgbm,]
predcomb <- predict(modelrf, agree)
combacc<- sum(predcomb == agree$y) / length(predcomb)
combacc
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
rf <- train(diagnosis ~ ., data = training, method="rf")
gbm <- train(diagnosis ~ ., data = training, method="gbm")
lda <- train(diagnosis ~ ., data = training, method="lda")
predrf <- predict(rf,testing)
predgbm <- predict(gbm,testing)
predlda <- predict(lda,testing)
predDF <- data.frame(predrf,predgbm,predlda,diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~.,method="gam",data=predDF)
combPred <- predict(combModFit,predDF)
rf_accuracy <- sum(predrf == testing$diagnosis) / length(predrf)
gbm_accuracy <- sum(predgbm == testing$diagnosis) / length(predgbm)
lda_accuracy <- sum(predlda == testing$diagnosis) / length(predlda)
all_accuracy <- sum(combPred == predF$diagnosis) / length(combPred)
all_accuracy <- sum(combPred == predDF$diagnosis) / length(combPred)
rf_accuracy; gbm_accuracy; lda_accuracy; all_accuracy;
combModFit <- train(diagnosis ~.,method="rf",data=predDF)
combPred <- predict(combModFit,predDF)
all_accuracy <- sum(combPred == predDF$diagnosis) / length(combPred)
all_accuracy;
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
?plot.enet
??plot.enet
install.packages('glmnet')
library(glmnet)
?plot.enet
install.packages('elasticnet')
library(elasticnet)
View(training)
modelasso <- train(CompressiveStrength~., data = training, method= "lasso")
plot.enet(modelasso, xvar = "penalty", use.color = TRUE)
plot.enet(modelasso$finalModel, xvar = "penalty", use.color = TRUE)
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
download.file(url, destfile = "./gaData.csv")
setwd("C:/Users/steph_000.STEPHANIE/Courseradatascience/Practical machine learning")
training$CompressiveStrength
url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"
download.file(url, destfile = "./gaData.csv")
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
library(lubridate)
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages('lubridate')
library(lubridate)
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages('forecast')
library(lubridate)
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
View(dat)
year(dat$date) < 2012
(year(dat$date)) > 2011
summary(training)
training$date
testing$date
year(dat$date) < 2012
year(dat$date) < 2011
year(dat$date) > 2011
?bats
??bats
library(forecast)
?bats
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
bats(tstrain)
model <- bats(tstrain)
forecast(model)
?forecast
fcast <- forecast(model)
accuracy(fcast, testing)
fcast
accuracy(fcast, testing)
accuracy(fcast,testing)
View(testing)
accuracy(fcast,x = testing)
plot(fcast)
model2 <- bats(training)
tstest = ts(testing$visitsTumblr)
accuracy(fcast,tstest)
accuracy(fcast,testing)
accuracy(fcast,testing$visitsTumblr)
library(e1071)
modelsvm <- svm(CompressiveStrength ~ ., data = training)
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
modelsvm <- svm(CompressiveStrength ~ ., data = training)
pred <- predict(modelsvm, newdata = testing)
sqrt(sum((pred-testing$CompressiveStrength)^2))
library(lubridate)
library(forecast)
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
tstest = ts(testing$visitsTumblr)
model <- bats(tstrain)
fcast <- forecast(model)
accuracy(fcast,testing$visitsTumblr)
# To finish
result <- c()
l <- length(fcast$lower)
for (i in 1:l){
x <- testing$visitsTumblr[i]
a <- fcast$lower[i] < x & x < fcast$upper[i]
result <- c(result, a)
}
sum(result)/l * 100
sqrt(mean(sum((pred-testing$CompressiveStrength))^2))
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
library(e1071)
modelsvm <- svm(CompressiveStrength ~ ., data = training)
pred <- predict(modelsvm, newdata = testing)
sqrt(mean(sum((pred-testing$CompressiveStrength))^2))
sqrt(mean(sum((pred-testing$CompressiveStrength)))^2)
